# 开启容灾恢复 Zone

NebulaGraph Operator 支持 Zone 功能。Zone 是一种逻辑概念，用于将集群中的 Storage Pod 分组。Zone 有助于提高集群的韧性，因为它可以确保数据副本在每个 Zone 中均匀分布。本文介绍如何创建带 Zone 的集群。

## 前提条件

已使用 NebulaGraph Operator 创建一个集群。有关如何创建集群的详细信息，请参见[创建集群](../4.1.installation/4.1.1.cluster-install.md)。

## 背景信息

{{nebula.name}}利用 Zone 的功能来高效管理其分布式架构。{{nebula.name}}的图空间数据被切分为不同分片，并且这些分片的副本均匀分布在所有可用的 Zone 中，同时查询可以优先发送到同一 Zone 内的 Storage Pod 中。使用 Zone 可显著降低 Zone 间的网络流量成本并提高数据传输速度。关于 Zone 的详细信息，请参见[管理 Zone](../../../4.deployment-and-installation/5.zone.md)。


## 配置 Zone

如要充分利用 Zone 功能，首先需要确定集群节点所在的实际 Zone。通常情况下，部署在云平台上的节点都带有各自 Zone 的标签。一旦获取了这些信息，可以通过在集群的配置文件中设置`spec.metad.config.zone_list`参数来进行配置。该参数是一个由逗号分隔的 Zone 名称列表，并且应与节点实际所在的 Zone 名称相匹配。例如，如果用户节点实际位于 az1、az2 和 az3 区域，配置应如下所示：

```yaml
spec:
  metad:
    config:
      zone_list: az1, az2, az3
```

NebulaGraph Operator 利用 Kubernetes 的 TopologySpread 功能来管理 Storage Pod 和 Graph Pod 的调度。一旦配置了`zone_list`，Storage Pod 将根据`topology.kubernetes.io/zone`标签自动分配到各自的 Zone 中。

## 优先访问同 Zone 数据

对于 Zone 内数据访问，Graph Pod 会使用`--assigned_zone=$NODE_ZONE`参数动态分配自己到一个 Zone 内。首先通过`spec.alpineImage`（默认值：`reg.example-inc.com/nebula-alpine:latest`）中指定的 Alpine Linux 镜像，可获取 Zone 信息。然后通过一个 init-container 来获取节点上的 Graph Pod 所在 Zone 的名称。

在集群配置文件中将`spec.graphd.config.prioritize_intra_zone_reading`设置为`true`，可以使 Graph Pod 优先将查询发送到同一 Zone 内的 Storage Pod。在该 Zone 内读取失败的情况下，行为取决于`spec.graphd.config.stick_to_intra_zone_on_failure`的值。如果设置为`true`，Graph 服务将避免从其他 Zone 内读取数据并返回错误。否则，它将读取其他 Zone 内分片的 Leader 副本数据。

```yaml
spec:
  alpineImage: reg.example-inc.com/xxx/xxx:latest
  graphd:
    config:
      prioritize_intra_zone_reading: "true"
      stick_to_intra_zone_on_failure: "false"
```

## 必配参数

如果需要创建带 Zone 的集群，必需在集群配置文件中添加以下字段。关于其他可选字段及描述，参见[创建集群](../4.1.installation/4.1.1.cluster-install.md)。

```yaml
spec:
  # 用来获取节点所在的 Zone 信息。
  alpineImage: "reg.example-inc.com/xxx/xxx:latest"
  graphd:
    image: reg.example-inc.com/xxx/xxx
    config:
      # 优先将查询发送到同一 Zone 内的存储节点。
      prioritize_intra_zone_reading: "true"
      stick_to_intra_zone_on_failure: "false" 
  metad:
    image: reg.example-inc.com/xxx/xxx
    config:
      # Zone 的名称列表，由逗号分隔。建议设置为奇数。
      zone_list: az1,az2,az3 
    licenseManagerURL: "192.168.8.xxx:9119"
  storaged:
    image: reg.example-inc.com/xxx/xxx
  imagePullSecrets:
  - name: nebula-image
  # 用于调度重启的 Graph/Storage Pods 至原来的 Zone。
  schedulerName: nebula-scheduler
  # 用于控制存储 Pod 分布的字段。
  topologySpreadConstraints:
  - topologyKey: "topology.kubernetes.io/zone"
    whenUnsatisfiable: "DoNotSchedule"
```

参数描述如下：

| 参数                             | 默认值                 | 描述                                                                                                       |
| ------------------------------ | -------------------- | -------------------------------------------------------------------------------------------------------- |
| `spec.metad.licenseManagerURL` | -                    | 配置指向许可证管理器（LM）的 URL，其中包括 LM 的访问地址和端口号（默认端口 `9119`）。例如，`192.168.8.xxx:9119`。**必须配置此参数以获取许可证信息；否则，无法使用企业版集群。** |
| `spec.<graphd|metad|storaged>.image`      | -                    | 企业版 Graph、Meta 或 Storage 服务的容器镜像。                                                                         |
| `spec.imagePullSecrets`        | -                    | 用于从私有仓库拉取 NebulaGraph 企业版服务镜像的 Secret。                                                                     |
| `spec.alpineImage`             | -                    | 用于获取节点所在 Zone 信息的 Alpine Linux 镜像。                                                                         |
| `spec.metad.config.zone_list`  | -                    | Zone 的名称列表，由逗号分隔。建议设置奇数个 Zone。例如：zone1,zone2,zone3。<br/>**一旦设置， Zone 名称无法更改。**                             |
| `spec.graphd.config.prioritize_intra_zone_reading` | `false` | 指定是否优先将查询发送到同一 Zone 内的存储节点。<br/>当设置为`true`时，查询将发送到同一 Zone 的存储节点。如果在该区 Zone 读取失败，它将根据`stick_to_intra_zone_on_failure`决定是否读数据分片的 Leader 副本数据。          |
| `spec.graphd.config.stick_to_intra_zone_on_failure` | `false` | 当设置为`false`时，如果在同一 Zone 内没有找到所请求数据时，将读取分片的 Leader 副本数据。当设置为`true`时，如果在该 Zone 内未读取到分片数据，将返回报错。|
| `schedulerName`| - | 调度器名称。使用 Zone 功能时，必须配置为`nebula-scheduler`，用于调度重启的 Graph 和 Storage Pods 至原始的 Zone。 |
| `spec.topologySpreadConstraints`              | -                    | 这是 Kubernetes 中用于控制 Storage Pod 分布的字段。其目的是确保其均匀分布在各个 Zone 中。<br/>**要使用 Zone 功能，必须将`topologySpreadConstraints[0].topologyKey`的值设置为`topology.kubernetes.io/zone`，并将`topologySpreadConstraints[0].whenUnsatisfiable`的值设置为`DoNotSchedule`**。运行`kubectl get node --show-labels`来检查该键信息。有关更多信息，请参阅[TopologySpread](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/#example-multiple-topologyspreadconstraints)。 |

!!! danger

    请勿手动修改 NebulaGraph Operator 创建的 ConfigMaps，否则可能会导致意外的行为。
    
    在 Storage Pod 和 Graph Pod 被分配到了各个 Zone 后，与其对应的 Zone 之间的映射信息将存储在名为`<cluster_name>-graphd|storaged-zone`的 ConfigMap 中。这个映射在滚动更新和 Pod 重启期间有助于 Pod 的调度，确保服务返回到所需的原始 Zone 中。

!!! caution

    创建集群后，导入数据前，请确保 Storage Pod 均匀分布在各个 Zone 中以提高集群的韧性。运行`SHOW ZONES`命令来检查 Storage Pod 是否均匀分布在各个 Zone 中。有关 Zone 相关命令，请参阅[管理 Zone](../../../4.deployment-and-installation/5.zone.md)。

## 带 Zone 的集群配置示例

### 使用`kubectl`创建带 Zone 的集群配置示例

以下为使用`kubectl`创建带 Zone 的集群 YAML 配置示例。

```yaml
apiVersion: apps.nebula-graph.io/v1alpha1
kind: NebulaCluster
metadata:
  name: nebula
  namespace: default
spec:
  # 用于获取节点所在位置的 Zone 信息。
  alpineImage: "reg.example-inc.com/xxx/xxx:latest"
  # 用于备份和恢复以及日志清理功能。
  # 如果你不自定义此配置，将使用默认配置。
  agent:
    image: reg.example-inc.com/xxx/xxx
    version: v{{nebula.release}}
  exporter:
    image: vesoft/nebula-stats-exporter
    replicas: 1
    maxRequests: 20
  # 用于创建一个控制台容器，用于连接到集群。
  console:
    version: "nightly"
  graphd:
    config:
      # 以下参数是创建具有 Zone 的集群所必需的。
      accept_partial_success: "true"
      prioritize_intra_zone_reading: "true"
      sync_meta_when_use_space: "true"
      stick_to_intra_zone_on_failure: "false" 
      session_reclaim_interval_secs: "300"
      # 以下参数是收集日志所必需的。
      logtostderr: "1"
      redirect_stdout: "false"
      stderrthreshold: "0" 
    resources:
      requests:
        cpu: "2"
        memory: "2Gi"
      limits:
        cpu: "2"
        memory: "2Gi"
    replicas: 1
    image: reg.example-inc.com/xxx/xxx
    version: v3.5.0-sc
  metad:
    config:
      redirect_stdout: "false"
      stderrthreshold: "0"
      logtostder: "true"
      # Zone 名称一旦设置就不能修改。
      # 建议设置奇数个 Zone。
      zone_list: az1,az2,az3 
      validate_session_timestamp: "false"
    # LM 访问地址和端口号。
    licenseManagerURL: "192.168.8.xxx:9119"
    resources:
      requests:
        cpu: "300m"
        memory: "500Mi"
      limits:
        cpu: "1"
        memory: "1Gi"
    replicas: 3
    image: reg.example-inc.com/xxx/xxx
    version: v3.5.0-sc
    dataVolumeClaim:
      resources:
        requests:
          storage: 2Gi
      storageClassName: local-path
  storaged:
    config:
      redirect_stdout: "false"
      stderrthreshold: "0"
      logtostder: "true"
    resources:
      requests:
        cpu: "1"
        memory: "1Gi"
      limits:
        cpu: "2"
        memory: "2Gi"
    replicas: 3
    image: reg.example-inc.com/xxx/xxx
    version: v3.5.0-sc
    dataVolumeClaims:
    - resources:
        requests:
          storage: 2Gi
      storageClassName: local-path
    # 扩容后自动平衡存储数据。
    enableAutoBalance: true
  reference:
    name: statefulsets.apps
    version: v1
  schedulerName: nebula-scheduler
  nodeSelector:
    nebula: cloud
  imagePullPolicy: Always
  imagePullSecrets:
  - name: nebula-image
  # 在 Zone 之间均匀分布存储 Pods。
  # 使用 Zone 时必须设置。
  topologySpreadConstraints:
  - topologyKey: "topology.kubernetes.io/zone"
    whenUnsatisfiable: "DoNotSchedule"
```

### 使用`helm`创建带 Zone 的集群配置示例

以下为使用`helm`创建带 Zone 的集群示例。

```bash
helm install "<NEBULA_CLUSTER_NAME>" nebula-operator/nebula-cluster \
    # 指定集群 chart 的版本，不指定则默认安装最新版本 chart。
    # 执行 helm search repo nebula-operator/nebula-cluster 命令可查看所有 chart 版本。
    --version={{operator.release}} \
    # 指定集群的命名空间。
    --namespace="<NEBULA_CLUSTER_NAMESPACE>" \
    # 配置拉取私有仓库中镜像的 Secret。
    --set imagePullSecrets[0].name="{<image-pull-secret>}" \
    --set nameOverride="<NEBULA_CLUSTER_NAME>" \
    # 配置指向 LM 访问地址和端口，默认端口为`9119`。必须配置此参数以获取 License 信息。
    --set nebula.metad.licenseManagerURL="192.168.8.XXX:9119" \
    # 配置集群中各服务的镜像地址。
    --set nebula.graphd.image="<reg.example-inc.com/test/graphd-ent>" \
    --set nebula.metad.image="<reg.example-inc.com/test/metad-ent>" \
    --set nebula.storaged.image="<reg.example-inc.com/test/storaged-ent>" \
    --set nebula.storageClassName="<STORAGE_CLASS_NAME>" \
    # 指定{{nebula.name}}集群的版本。
    --set nebula.version=v{{nebula.release}} \
    # 配置 Zone。
    # 配置 Zone 后，Zone 的信息不能修改。建议配置奇数个 Zone。
    --set nebula.metad.config.zone_list="<zone1,zone2,zone3>" \
    --set nebula.graphd.config.prioritize_intra_zone_reading="true" \
    --set nebula.graphd.config.stick_to_intra_zone_on_failure="false" \
    # 配置 Alpine Linux 镜像，用于获取节点所在 Zone 信息。
    --set nebula.alpineImage="<reg.example-inc.com/xxx/xxx:latest>" \
    # 设置均分 Storage Pod 到不同 Zone。
    --set nebula.topologySpreadConstraints[0].topologyKey="topology.kubernetes.io/zone" \
    --set nebula.topologySpreadConstraints[0].whenUnsatisfiable="DoNotSchedule" \
    # 调度重启的 Graph/Storage Pods 至原来的 Zone。
    --set nebula.schedulerName="nebula-scheduler"
```  

